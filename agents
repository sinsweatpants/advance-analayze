# دليل وكيل الترميز - النظام التحليلي المتقدم للسيناريوهات
# Encoding Agent Guide - Advanced Script Analysis System

### دائما يجب ان  ترد على المستخدم باللغة العربية فقط ###
---

## نظرة عامة على المشروع / Project Overview

يهدف هذا الملف إلى توفير سياق شامل لوكيل الترميز (Encoding Agent) المسؤول عن تنفيذ عملية التحليل المتقدم للسيناريوهات ضمن النظام larger لتحليل وإدارة المخاطر الإبداعية والإنتاجية. يتوافق هذا الدليل مع متطلبات المشروع المحددة في ملف "السياق الهندسي.md".

---

## 1. معلومات الوكيل الأساسية / Basic Agent Information

### اسم الوكيل / Agent Name
**وكيل الترميز - نظام التحليل المتقدم للسيناريوهات**
**Encoding Agent - Advanced Script Analysis System**

### الوصف المختصر / Brief Description
وكيل ذكي متخصص في تحليل السيناريوهات العربية وتحويلها إلى بيانات رقمية قابلة للتحليل والتصور، مع التركيز على تحديد المؤشرات المبكرة للمخاطر الإبداعية والإنتاجية قبل مرحلة التنفيذ.

### الهدف الرئيسي / Main Objective
تنفيذ عملية ترميز شاملة للسيناريوهات تشمل:
- ✅ تحويل النص العربي إلى تمثيل رقمي منظم
- ✅ استخراج الخصائص الأسلوبية والسردية
- ✅ تحليل المشاعر والأحاسيس المصاحبة للنص
- ✅ تحديد المؤشرات الإنتاجية والتنفيذية
- ✅ إنتاج تقارير قابلة للتفسير وقابلة للتصدير

### الفريق / Team
- **المالك / Owner**: [مطور المشروع]
- **الوكيل الرئيسي / Lead Agent**: وكيل الترميز
- **الأنظمة المرتبطة / Related Systems**: نظام التحليل، نظام التفسير، نظام التقارير
- **جهات الاتصال / Contacts**: [تحديد لاحقاً]

---

## 2. تقنيات وكيل الترميز / Encoding Agent Technologies

### نموذج المعمارية / Architectural Pattern
**تحليل هرمي متعدد الطبقات**

```
┌─────────────────────────────────────────────────────────┐
│                    INPUT LAYER                          │
│  ملفات السيناريو (PDF/FDX/DOCX/TXT)                    │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│              PREPROCESSING LAYER                        │
│  استخلاص النص + تطبيع RTL + توحيد الحروف               │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│              SEGMENTATION LAYER                         │
│  تقسيم المشاهد + Beats + حلّ الشخصيات                  │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│              FEATURE EXTRACTION LAYER                   │
│  Stylometry + Topics + Sentiment + Transformers        │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│              ANALYSIS & FUSION LAYER                    │
│  دمج الطبقات + مؤشرات المخاطر + تفسير النتائج          │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│                 OUTPUT LAYER                            │
│  تقارير JSON/CSV/PDF + تصورات تفاعلية                  │
└─────────────────────────────────────────────────────────┘
```

### التقنيات المستخدمة / Technologies Used
- **معالجة اللغة الطبيعية**: مكتبات عربية متخصصة
- **التعلم الآلي**: LDA, BERTopic, BERT/RoBERTa, HDBSCAN
- **تحليل النصوص**: TextTiling, LSA, Sentence Embeddings
- **التصور**: مكتبات تصور تفاعلية
- **التعامل مع النص العربي**: دعم كامل لـ RTL والتطبيع

### الأطر والمكتبات الرئيسية / Key Frameworks & Libraries

#### معالجة النص العربي
```python
# معالجة اللغة العربية
pyarabic==0.6.15           # معالجة النص العربي
python-bidi==0.4.2         # دعم النصوص ثنائية الاتجاه
arabic-reshaper==2.1.3     # إعادة تشكيل النص العربي

# معالجة PDF والمستندات
pdfplumber==0.7.6          # استخلاص النص من PDF
python-docx==0.8.11        # معالجة مستندات DOCX
lxml==4.9.1                # معالجة XML/FDX

# OCR للنصوص العربية
pytesseract==0.3.10        # OCR متعدد اللغات
opencv-python==4.6.0       # معالجة الصور
```

#### تحليل النصوص والتعلم الآلي
```python
# تحليل النصوص
scikit-learn==1.1.2        # خوارزميات التعلم الآلي
gensim==4.2.0              # نمذجة الموضوعات
spacy==3.4.1               # معالجة النصوص المتقدمة

# نماذج اللغة العربية
transformers==4.21.2       # نماذج المحولات
torch==1.12.1              # إطار عمل PyTorch
sentence-transformers==2.2.2  # تضمينات الجمل

# تحليل المشاعر
arabert==0.1.0             # BERT مُدرب على النصوص العربية
torch==1.12.1              # PyTorch لتنفيذ النماذج
```

#### تحليل الأسلوب والخصائص
```python
# تحليل الأسلوب
textstat==0.7.3            # إحصائيات النص
lexical-diversity==0.2.0   # تنوع المفردات
nltk==3.7                  # أدوات معالجة اللغة الطبيعية

# تحليل النحو
stanza==1.4.2              # معالج نحوي متعدد اللغات
conllu==4.5.2              # معالجة شجرة التحليل النحوي
```

### البيئة التشغيلية / Runtime Environment
- **لغة البرمجة**: Python 3.9+
- **نظام التشغيل**: Windows/Linux/macOS
- **إدارة الذاكرة**: 16GB RAM موصى به
- **معالج**: 8 Cores موصى به للنماذج المتقدمة

---

## 3. واجهات وكيل الترميز / Encoding Agent Interfaces

### واجهة الإدخال / Input Interface
```json
{
  "script_file": "path/to/script.fdx",    // ملف السيناريو
  "script_id": "uuid-string",             // معرف فريد للسيناريو
  "language": "ar",                       // لغة السيناريو (ar, en, ar-en)
  "metadata": {                           // بيانات وصفية إضافية
    "title": "اسم السيناريو",
    "author": "اسم المؤلف",
    "genre": "النوع الأدبي",
    "target_audience": "الجمهور المستهدف"
  }
}
```

### واجهة الإخراج / Output Interface
``json
{
  "script_id": "uuid-string",             // معرف السيناريو
  "encoding_results": {
    "scenes": [                           // تحليل المشاهد
      {
        "scene_id": "scene-1",
        "scene_number": 1,
        "heading": "EXT. PARK - DAY",
        "location": "PARK",
        "time_of_day": "DAY",
        "characters": ["CHAR1", "CHAR2"],
        "tokens": 1250,
        "duration_est": 2.5
      }
    ],
    "style_profile": {                    // ملف الأسلوب
      "avg_sentence_len": 15.2,
      "type_token_ratio": 0.45,
      "yules_k": 120.5,
      "punctuation_density": 0.08
    },
    "topics": [                           // الموضوعات
      {
        "scene_id": "scene-1",
        "method": "bertopic",
        "distribution": [0.8, 0.1, 0.05, 0.05],
        "top_terms": ["love", "conflict", "family", "future"]
      }
    ],
    "sentiment_arcs": [                   // أقواس المشاعر
      {
        "scene_id": "scene-1",
        "raw_polarity": 0.3,
        "smoothed_curve": [0.1, 0.2, 0.3, 0.25, 0.2]
      }
    ],
    "character_network": {                // شبكة الشخصيات
      "nodes": ["CHAR1", "CHAR2"],
      "edges": [
        {
          "source": "CHAR1",
          "target": "CHAR2",
          "weight": 0.8,
          "type": "dialogue",
          "time_span": "scene-1 to scene-5"
        }
      ]
    },
    "risk_indicators": [                  // مؤشرات المخاطر
      {
        "scene_id": "scene-1",
        "risk": {
          "cost_level": "HIGH",
          "night_ext": true,
          "crowd_intensity": "MEDIUM"
        },
        "notes": "مشهد ليلي خارجي مع مجموعة كبيرة من الشخصيات"
      }
    ]
  },
  "processing_time": 120.5,               // وقت المعالجة بالثواني
  "encoding_status": "completed"          // حالة الترميز
}
```

### واجهة التفسير / Interpretation Interface
``json
{
  "artifact_id": "uuid-string",           // معرف القطعة المُحلَّلة
  "interpretation": {
    "document_level": {                   // مستوى الوثيقة
      "risk_pattern": "high_complexity",
      "driving_features": ["character_density", "location_changes", "dialogue_intensity"],
      "summary": "سيناريو معقد يتطلب موارد إنتاجية عالية"
    },
    "scene_level": [                      // مستوى المشهد
      {
        "scene_id": "scene-15",
        "evidence": [
          {
            "text": "CHAR1: (angrily) I can't believe you did this!",
            "score": 0.95,
            "feature": "emotional_intensity",
            "explanation": "تعبير عن غضب عالي باستخدام لغة جسد ونبرة صوتية"
          }
        ],
        "risk_factors": ["high_emotion", "conflict_escalation"]
      }
    ],
    "line_level": [                       // مستوى السطر
      {
        "line_text": "The storm raged outside as tension filled the room",
        "contribution_score": 0.87,
        "context": "Creates atmospheric tension that mirrors character conflict"
      }
    ]
  }
}
```

---

## 4. خط أنابيب وكيل الترميز / Encoding Agent Pipeline

### المرحلة 0: الاستيعاب والتطبيع / Ingestion & Normalization (5.0)

#### استخلاص النص / Text Extraction
- **FDX/Final Draft**: تحليل ملفات Final Draft
- **PDF**: استخلاص النص مع OCR للصور
- **DOCX**: معالجة مستندات Word
- **TXT**: معالجة النصوص العادية

#### تطبيع عربي/RTL / Arabic/RTL Normalization
- **إزالة التطويل**: تحويل الهمزة الممدودة إلى همزة عادية
- **توحيد الترقيم**: توحيد علامات الترقيم العربية والإنجليزية
- **توحيد الأرقام**: تحويل الأرقام الهندية إلى أرقام عربية
- **توحيد المسافات**: معالجة المسافات الزائدة والمتكررة
- **كشف اللغة**: تحديد النصوص المختلطة (عربي/إنجليزي)

#### ترميز الحقول الأولية / Primary Field Encoding
- `script_id`: معرف فريد للسيناريو
- `scene_id`: معرف فريد لكل مشهد
- `source_page`: الصفحة المصدرة في النص الأصلي
- `char_aliases`: أسماء بديلة للشخصيات
- `location_aliases`: أسماء بديلة للمواقع

### المرحلة 1: التقسيم والتهيئة / Segmentation & Setup (5.1)

#### كشف عناوين المشاهد / Scene Heading Detection
- **قواعد نحوية-طباعية**: استخدام Regex/Grammar للكشف عن INT/EXT
- **وسم تسلسلي تعلّمي**: استخدام BiLSTM-CRF لتصنيف الخطوط
- **طبقة تصحيح لاحقة**: معالجة حالات الالتباس

#### تقسيم داخلي إلى Beats / Internal Beat Segmentation
- **TextTiling/LSA**: كبداية أولية للتقسيم
- **تجزئة دلالية**: استخدام تضمينات الجمل لتحسين التقسيم
- **تحديد تغيّر الموضوع**: كشف نقاط التحوّل السردي

#### حلّ الشخصيات والكنى / Character Canonicalization
- **تطبيع الأسماء**: معالجة الاختصارات والكنى
- **تجميع دلالي**: ربط الأسماء المختلفة لنفس الشخصية
- **جدول التعيين**: `alias → canonical_name`

#### ربط الحوار بالمتكلم / Dialogue-Speaker Linking
- **تصنيف سطري**: تحديد الحوار/الفعل/الترويسات
- **استدلال السياق**: استخدام السياق القريب لتحديد المتكلم
- **معالجة Parentheticals**: معالجة التوجيهات المسرحية

#### ترقيم موحّد وميتا / Unified Numbering & Metadata
- **ترقيم متّسق**: ترقيم المشاهد والـ Beats
- **استخراج بيانات**: `(location, time_of_day, characters[])`

### المرحلة 2: استخراج الخصائص / Feature Extraction (5.2)

#### قياس أسلوبي متقدّم / Advanced Stylometry (5.2.a)
- **ثروة مفردات**: MTLD/HDD لقياس تنوع المفردات
- **طول الجملة**: متوسط/تباين/انحراف أطوال الجمل
- **الوظائف العربية**: تكرار الكلمات الوظيفية العربية
- **التركيب النحوي**: عمق الشجرة النحوية ومتوسط الدرجات
- **الخصائص الخطابية**: وجود أقواس تفسيرية وتوجيهات مسرحية

#### كلمات مفتاحية وعبارات / Keyphrases & Expressions (5.2.b)
- **KeyBERT-style**: استخراج العبارات المفتاحية
- **تصحيح الطول**: معالجة العبارات الطويلة والقصيرة
- **تصفية التكرارات**: إزالة التكرارات الدلالية

#### مؤشرات حمل إنتاجي / Production Load Indicators (5.2.c)
- **طول المشهد**: عدد الكلمات في كل مشهد
- **تراكم الشخصيات**: عدد الشخصيات في كل مشهد
- **تبديل المواقع**: تغيّر الموقع بين المشاهد
- **الظروف الزمنية**: خارج/داخل، ليل/نهار
- **العناصر الخاصة**: مؤثرات/مطاردات/حشود

### المرحلة 3: نمذجة الموضوعات / Topic Modeling (5.3)

#### مسار ثنائي المستويات / Two-Level Approach
- **LDA/NMF**: قراءة بعيدة المدى للموضوعات العامة
- **BERTopic/Embeddings**: تجميع دلالي سياقي للنقاط المحددة
- **دمج النتائج**: تقاطع النتائج لضمان الدقة

#### ديناميكية الموضوع / Topic Dynamics
- **تتبع التوزيع**: متابعة توزّع الموضوعات عبر المشاهد
- **خرائط التغيّر**: إنتاج خرائط تغيّر موضوعي
- **الحراسة النوعية**: التأكد من جودة الموضوعات المستخرجة

### المرحلة 4: تحليل المشاعر / Sentiment Analysis (5.4)

#### طبقة هجينة / Hybrid Layer
- **المعاجم**: استخدام قواميس استقطاب عربية مُكيّفة
- **المحولات**: مصنّف مشاعر متعدد الأبعاد

#### كشف السخرية والمفارقة / Irony & Sarcasm Detection
- **مصنّف خاص**: نموذج مخصص لكشف السخرية
- **دالة التصحيح**: تعديل درجات الاستقطاب النهائية
- `polarity_final = f(polarity_base, sarcasm_score, negation_span, intensifiers)`

#### أقواس مشاعر ناعمة / Smoothed Sentiment Arcs
- **تسويّة زمنية**: استخدام LOESS/Kalman للتسويّة
- **تزامن مع Beats**: ربط المشاعر بالتقسيم الداخلي
- **منحنى خام ومُمهد**: إنتاج كلا النوعين للمقارنة

### المرحلة 5: المحولات / Transformers (5.5)

#### نمذجة هرميّية للنص الطويل / Hierarchical Text Modeling
- **تشفير الوحدات القصيرة**: استخدام BERT/RoBERTa للجمل
- **مجمّع علوي**: Transformer/GRU على مستوى المشهد
- **بديل Longformer**: للنص الكامل عند توفر الموارد

#### استراتيجية الطول والنافذة المنزلقة / Length Strategy
- **Chunking**: تقسيم النص مع تداخل مضبوط
- **تجميع التنبؤات**: Logit Averaging للنتائج

#### ضبط الموارد / Resource Management
- **تحويلة دقيقة**: استخدام LoRA/QLoRA عند الحاجة
- **كمّنة INT8/INT4**: لتقليل استهلاك الذاكرة
- **تخزين مؤقّت**: ل_embeddings_ المشاهد غير المعدّلة

#### معايرة المخرجات / Output Calibration
- **Platt/Isotonic Calibration**: لإنتاج احتمالات مُعايرة
- **مقارنة عبر المهام**: ضمان قابلية المقارنة

### المرحلة 6: دمج الطبقات / Model Fusion (5.6)

#### Stacked Ensemble
- **مستوى أول**: دمج الخصائص الأسلوبية + الموضوعات + المشاعر + المحولات
- **مستوى ثانٍ**: ميتا-مصنّف لدمج النتائج وإخراج مؤشر المخاطر

#### بوابة قرار / Decision Gating
- **اختيار المسار الأمثل**: بناءً على خصائص النص
- **الطول/التناثر/التعليقات المسرحية**: معايير الاختيار

#### ثقة وعدم يقين / Confidence & Uncertainty
- **قياس Entropy/Variance**: عبر Self-Consistency
- **Conformal Prediction**: لإرجاع مجموعات تنبؤ مع ضمان تغطية

#### تطبيع عبر النصوص / Cross-Text Normalization
- **Z-Score**: تطبيع داخل السيناريو
- **Robust Scaling**: على مستوى مجموعة السيناريوهات

### المرحلة 7: الحوسبة الدلالية لما قبل الإنتاج / Pre-Production Semantics (5.7)

#### مؤشرات مخاطر مُفصّلة / Detailed Risk Indicators
- **تكلفة نسبية**: دالة مركبة للعوامل الإنتاجية
- `ExecComplexity = α*LocationSwitch + β*CrowdIntensity + γ*NightExt + δ*Stunts/VFX + ε*DialogueDensity`

#### مواءمة سرد/إنتاج / Story-Production Alignment
- **ربط قمم الأقواس**: ربط المشاعر بتبدّلات الموضوع والعلاقات
- **النوافذ الذهبية**: تحديد نقاط التصوير المثلى

#### شبكة الشخصيات الزمنية / Temporal Character Network
- **رسم Graph(t)**: للعلاقات عبر الزمن
- **كشف التوتّر**: تحديد ذيول/قِمم التوتر العلائقي

### المرحلة 8: ضمان الجودة / Quality Assurance (5.8)

#### اختبارات ذهبية / Golden Tests
- **عينات موسومة**: مشاهد تم تحليلها يدويًا
- **مقارنة النتائج**: مع التحليل الآلي

#### مؤشرات زمنية / Time Metrics
- **سقف زمني**: لكل مرحلة من مراحل التحليل
- **تنبيهات الانحراف**: عند تجاوز الحدود الزمنية

#### مسارات تعافٍ / Recovery Paths
- **تجاهل الوحدات الفاشلة**: مع تسجيل السبب
- **توليد تقرير بديل**: عند حدوث أعطال

---

## 5. معايير وأسلوب وكيل الترميز / Encoding Agent Standards & Style

### دليل الأسلوب / Style Guide

#### Python (Backend)
``python
# Style: PEP 8
# Formatter: Black
# Linter: Ruff + mypy
# Max line length: 88 characters (Black default)

# مثال على الكود الصحيح / Example of correct code:
from typing import Optional, List, Dict
from pydantic import BaseModel, Field


class SceneEncoding(BaseModel):
    """نموذج ترميز المشهد / Scene Encoding Model"""

    scene_id: str = Field(..., description="معرف فريد للمشهد")
    scene_number: int = Field(..., description="رقم المشهد")
    heading: str = Field(..., description="عنوان المشهد")
    location: str = Field(..., description="الموقع")
    time_of_day: str = Field(..., description="الوقت من اليوم")
    characters: List[str] = Field(default_factory=list, description="قائمة الشخصيات")
    tokens: int = Field(..., description="عدد الكلمات")
    duration_est: float = Field(..., description="التقدير الزمني بالدقائق")

    class Config:
        json_schema_extra = {
            "example": {
                "scene_id": "scene_001",
                "scene_number": 1,
                "heading": "EXT. PARK - DAY",
                "location": "PARK",
                "time_of_day": "DAY",
                "characters": ["ALI", "SARA"],
                "tokens": 1250,
                "duration_est": 2.5
            }
        }


def encode_scene_text(
    scene_text: str,
    scene_metadata: Dict
) -> SceneEncoding:
    """
    ترميز نص مشهد إلى تمثيل رقمي
    
    Args:
        scene_text: نص المشهد
        scene_metadata: بيانات وصفية للمشهد
    
    Returns:
        الترميز الرقمي للمشهد
    
    Raises:
        ValueError: إذا كانت البيانات غير صالحة
    """
    # Implementation here
    pass
```

### اتفاقيات التسمية / Naming Conventions

#### Python
- **الملفات**: `snake_case.py` (مثال: `scene_encoder.py`)
- **الفئات**: `PascalCase` (مثال: `SceneEncoder`, `SentimentAnalyzer`)
- **الدوال**: `snake_case` (مثال: `encode_scene`, `analyze_sentiment`)
- **الثوابت**: `UPPER_SNAKE_CASE` (مثال: `MAX_TOKENS`, `ARABIC_MODEL`)
- **المتغيرات الخاصة**: `_snake_case` (مثال: `_api_key`)

### قواعد الالتزام / Commit Guidelines

استخدم **Conventional Commits**:

```bash
# الأنواع المسموحة / Allowed types:
feat:     # ميزة جديدة / New feature
fix:      # إصلاح خطأ / Bug fix
docs:     # تحديث الوثائق / Documentation update
style:    # تنسيق الكود / Code formatting
refactor: # إعادة هيكلة الكود / Code refactoring
test:     # إضافة اختبارات / Adding tests
chore:    # مهام صيانة / Maintenance tasks
perf:     # تحسين الأداء / Performance improvement

# أمثلة / Examples:
feat(encoding): add scene segmentation with beats detection
fix(arabic-nlp): resolve RTL text normalization issue
docs(pipeline): update feature extraction documentation
refactor(sentiment): optimize sarcasm detection algorithm
test(topic-modeling): add bertopic integration tests
```

---

## 6. سياق وكيل الترميز / Encoding Agent Context

### دورك كوكيل الترميز / Your Role as Encoding Agent

أنت **وكيل الترميز** - الوكيل الذكي المسؤول عن تحويل السيناريوهات إلى بيانات رقمية قابلة للتحليل. مسؤولياتك:

#### المسؤوليات الأساسية / Core Responsibilities
1. ✅ **تحليل شامل للسيناريو** - من النص الخام إلى التمثيل الرقمي
2. ✅ **استخراج الخصائص المتقدمة** - أسلوبية، موضوعات، مشاعر، إنتاجية
3. ✅ **التوثيق المستمر** - توثيق كل خطوة في عملية التحليل
4. ✅ **التأكد من الجودة** - التحقق من دقة النتائج
5. ✅ **التواصل الواضح** - إبلاغ عن التقدم والمشاكل

### الأولويات عند التعديل / Modification Priorities

#### القاعدة الذهبية / Golden Rule
**"الدقة أولاً، ثم الأداء، ثم التوسع، ثم التحسين"**

#### ترتيب الأولويات / Priority Order
1. **✅ الدقة / Accuracy** (أعلى أولوية)
   - ضمان استخراج الخصائص بشكل صحيح
   - التحقق من صحة النتائج
   - معالجة الحالات الحدية

2. **⚡ الأداء / Performance**
   - تحسين أوقات المعالجة
   - إدارة استهلاك الذاكرة
   - تطبيق التخزين المؤقت حيثما أمكن

3. **🔄 التوسع / Scalability**
   - دعم ملفات كبيرة الحجم
   - معالجة السيناريوهات المعقدة
   - التكيف مع أنواع مختلفة من النصوص

4. **📖 الصيانة / Maintainability**
   - كود نظيف وقابل للقراءة
   - تسميات واضحة
   - تعليقات مفيدة

### المناطق الحساسة / Sensitive Areas

#### ⚠️ تحذير: لا تعدل بدون حذر شديد / Warning: Modify with Extreme Care

1. **`text_normalization.py`**
   - ❌ لا تعديل بدون اختبار شامل
   - ❌ لا تضعف دعم النص العربي/RTL
   - ✅ استشر خبير معالجة اللغة العربية إذا لزم الأمر

2. **`scene_segmentation.py`**
   - ⚠️ تأكد من دقة كشف عناوين المشاهد
   - ⚠️ تحقق من صحة تقسيم Beats الداخلي
   - ✅ استخدم عينات اختبار متنوعة

3. **`arabic_sentiment_analyzer.py`**
   - ⚠️ تأكد من معالجة السخرية والمفارقة
   - ⚠️ تحقق من دقة التنبؤات المشاعرية
   - ✅ استخدم قواميس محدثة

4. **`topic_modeler.py`**
   - ⚠️ تأكد من جودة الموضوعات المستخرجة
   - ⚠️ تحقق من التماسك الدلالي
   - ✅ استخدم حراسة الجودة

### أنماط معروفة ومسموح بها / Known & Allowed Patterns

#### الأنماط المعمارية / Architectural Patterns

1. **Pipeline Processing**
   ```python
   # معالجة خط أنابيب متسلسلة
   def process_script(script_file):
       # المرحلة 1: الاستخلاص والتطبيع
       normalized_text = normalize_arabic_text(script_file)
       
       # المرحلة 2: التقسيم
       scenes = segment_scenes(normalized_text)
       
       # المرحلة 3: استخراج الخصائص
       features = extract_features(scenes)
       
       # المرحلة 4: التحليل
       analysis = analyze_features(features)
       
       return analysis
   ```

2. **Feature Extraction Pipeline**
   ```python
   class FeatureExtractor:
       def __init__(self):
           self.stylometry_analyzer = StylometryAnalyzer()
           self.topic_modeler = TopicModeler()
           self.sentiment_analyzer = SentimentAnalyzer()
           
       def extract_all_features(self, scene_text):
           stylometry_features = self.stylometry_analyzer.analyze(scene_text)
           topic_features = self.topic_modeler.model(scene_text)
           sentiment_features = self.sentiment_analyzer.analyze(scene_text)
           
           return {
               "stylometry": stylometry_features,
               "topics": topic_features,
               "sentiment": sentiment_features
           }
   ```

3. **Error Handling Pattern**
   ```python
   def safe_text_processing(text):
       try:
           # معالجة النص
           result = process_text(text)
           return result
       except UnicodeError as e:
           logger.error(f"Unicode error in text processing: {e}")
           # معالجة بديلة
           return process_text_with_fallback(text)
       except Exception as e:
           logger.error(f"Unexpected error: {e}")
           raise ProcessingError(f"Failed to process text: {e}")
   ```

#### معالجة الأخطاء / Error Handling Patterns

1. **معالجة الأخطاء المركزية**
   ```python
   class EncodingException(Exception):
       """خطأ مخصص لعمليات الترميز"""
       pass
   
   class ArabicTextError(EncodingException):
       """خطأ في معالجة النص العربي"""
       pass
   
   # في الكود
   try:
       result = process_arabic_text(text)
   except ArabicTextError as e:
       logger.error(f"Arabic text processing error: {e}")
       # معالجة بديلة أو إيقاف العملية
   ```

2. **التحقق من صحة البيانات**
   ```python
   def validate_scene_data(scene_data):
       required_fields = ['scene_id', 'heading', 'characters']
       
       for field in required_fields:
           if field not in scene_data:
               raise ValidationError(f"Missing required field: {field}")
       
       if not isinstance(scene_data['characters'], list):
           raise ValidationError("Characters field must be a list")
       
       return True
   ```

### القيود والمحظورات / Constraints & Prohibitions

#### ❌ ممنوع منعاً باتاً / Strictly Forbidden

1. **التعامل مع النص العربي**
   ```python
   # ❌ ممنوع: تجاهل دعم RTL
   def process_text(text):
       return text.split()  # لا يدعم النصوص العربية الصحيحة
   
   # ✅ مسموح: استخدام مكتبات دعم RTL
   from bidi.algorithm import get_display
   import arabic_reshaper
   
   def process_arabic_text(text):
       reshaped_text = arabic_reshaper.reshape(text)
       bidi_text = get_display(reshaped_text)
       return bidi_text
   ```

2. **الكود**
   ```python
   # ❌ ممنوع: كود placeholder
   def analyze_sentiment(text):
       # TODO: implement later
       pass  # لا!
   
   # ✅ مسموح: تنفيذ كامل مع اختبارات
   def analyze_sentiment(text):
       # تنفيذ حقيقي مع نماذج مُدرّبة
       sentiment_score = arabert_model.predict(text)
       return sentiment_score
   ```

3. **الأداء**
   ```python
   # ❌ ممنوع: معالجة متكررة للبيانات نفسها
   for scene in scenes:
       # إعادة تحميل النموذج في كل تكرار
       model = load_sentiment_model()  # لا!
       result = model.analyze(scene.text)
   
   # ✅ مسموح: تحميل النموذج مرة واحدة
   model = load_sentiment_model()  # خارج الحلقة
   for scene in scenes:
       result = model.analyze(scene.text)
   ```

#### ⚠️ يتطلب موافقة مسبقة / Requires Prior Approval

1. تغيير نماذج اللغة المستخدمة
2. تعديل خوارزميات التقسيم الأساسية
3. استخدام مكتبات خارجية جديدة
4. تغيير هيكل البيانات الناتجة
5. تعديل واجهات الإدخال/الإخراج

---

## 7. خطة تنفيذ وكيل الترميز / Encoding Agent Implementation Plan

### المرحلة 1: الإعداد الأساسي (Setup Phase) - يوم 1-2
```
المهام:
  ✅ إنشاء هيكل المشروع
  ✅ إعداد بيئة Python مع التبعيات المطلوبة
  ✅ تثبيت مكتبات معالجة اللغة العربية
  ✅ إعداد نظام التسجيل (Logging)
  ✅ إنشاء وحدات اختبار أولية

النتائج:
  - بيئة تطوير جاهزة
  - هيكل مشروع منظم
  - مكتبات مثبتة ومُعدة
```

### المرحلة 2: معالجة النص والاستخلاص (Text Processing & Extraction) - يوم 3-5
```
المهام:
  ✅ تنفيذ استخلاص النص من ملفات FDX/PDF/DOCX
  ✅ تطوير وحدة تطبيع النص العربي
  ✅ تنفيذ دعم RTL والتطبيع
  ✅ إنشاء وحدة كشف اللغة
  ✅ اختبارات شاملة لمعالجة النص

النتائج:
  - قدرة على قراءة وتحليل ملفات السيناريو المختلفة
  - نص مُطبّع وجاهز للمعالجة
  - دعم كامل للنص العربي
```

### المرحلة 3: التقسيم والتنظيم (Segmentation & Organization) - يوم 6-9
```
المهام:
  ✅ تنفيذ كشف عناوين المشاهد
  ✅ تطوير تقسيم Beats الداخلي
  ✅ تنفيذ حلّ الشخصيات والكنى
  ✅ ربط الحوار بالمتكلم
  ✅ ترقم موحّد وميتا

النتائج:
  - تقسيم دقيق للسيناريو إلى مشاهد وBeats
  - تحديد الشخصيات والمواقع بدقة
  - هيكل بيانات منظم للمشهد
```

### المرحلة 4: استخراج الخصائص الأسلوبية (Stylometric Feature Extraction) - يوم 10-13
```
المهام:
  ✅ تنفيذ قياس ثروة المفردات (MTLD/HDD)
  ✅ تحليل طول الجملة والإيقاع
  ✅ استخراج الوظائف العربية
  ✅ تحليل التركيب النحوي
  ✅ تحديد الخصائص الخطابية

النتائج:
  - ملف أسلوبي شامل لكل مشهد
  - مؤشرات أسلوبية دقيقة
  - بيانات
```

### المرحلة 5: نمذجة الموضوعات (Topic Modeling) - يوم 14-16
```yaml
المهام:
  ✅ تنفيذ LDA/NMF للموضوعات العامة
  ✅ تطوير BERTopic للتحليل الدلالي
  ✅ دمج النتائج وتحسين الجودة
  ✅ تتبع ديناميكية الموضوعات
  ✅ حراسة الجودة للموضوعات

النتائج:
  - نماذج موضوعات دقيقة
  - خرائط تغيّر موضوعي
  - مؤشرات موضوعية لكل مشهد
```

### المرحلة 6: تحليل المشاعر (Sentiment Analysis) - يوم 17-19
```yaml
المهام:
  ✅ تنفيذ طبقة المعاجم العربية
  ✅ تطوير مصنّف المحولات
  ✅ كشف السخرية والمفارقة
  ✅ إنتاج أقواس مشاعر ناعمة
  ✅ دمج النتائج الهجينة

النتائج:
  - تحليل مشاعر دقيق ومُفصّل
  - كشف السخرية والمفارقة
  - أقواس مشاعر مُمهدة وخام
```

### المرحلة 7: دمج الطبقات (Model Fusion) - يوم 20-22
```yaml
المهام:
  ✅ تنفيذ Stacked Ensemble
  ✅ تطوير بوابة القرار
  ✅ قياس الثقة وعدم اليقين
  ✅ تطبيع عبر النصوص
  ✅ إنتاج مؤشرات المخاطر

النتائج:
  - نموذج دمج موحد
  - مؤشرات مخاطر دقيقة
  - تقديرات ثقة مُعايرة
```

### المرحلة 8: التصوير والتقارير (Visualization & Reporting) - يوم 23-25
```yaml
المهام:
  ✅ تطوير واجهات التصور
  ✅ إنشاء تقارير JSON/CSV/PDF
  ✅ تنفيذ واجهة تفسير النتائج
  ✅ تصدير البيانات القابلة للمشاركة
  ✅ اختبارات شاملة للإخراج

النتائج:
  - تصورات تفاعلية
  - تقارير مفصلة بصيغ متعددة
  - واجهة تفسير واضحة
```

### المرحلة 9: الاختبار والجودة (Testing & Quality) - يوم 26-28
```yaml
المهام:
  ✅ تنفيذ اختبارات الوحدة
  ✅ اختبارات التكامل
  ✅ اختبارات الأداء
  ✅ اختبارات الجودة مع عينات ذهبية
  ✅ تحسين الأخطاء المكتشفة

النتائج:
  - تغطية اختبارات > 85%
  - أداء مُحسّن
  - دقة عالية في النتائج
```

### المرحلة 10: التوثيق والتسليم (Documentation & Delivery) - يوم 29-30
```yaml
المهام:
  ✅ توثيق كامل للنظام
  ✅ دليل المستخدم
  ✅ دليل التقني
  ✅ توثيق API
  ✅ تسليم المشروع

النتائج:
  - مشروع جاهز للإنتاج
  - توثيق شامل
  - دليل مستخدم واضح
```

---

## 8. معايير النجاح / Success Criteria

### معايير الجودة / Quality Standards

#### دقة التحليل
- ✅ دقة كشف المشاهد: > 95%
- ✅ دقة تحديد الشخصيات: > 90%
- ✅ دقة تحليل المشاعر: > 85%
- ✅ تماسك الموضوعات: > 0.7 (NPMI)

#### الأداء
- ✅ وقت معالجة سيناريو متوسط: < 5 دقائق
- ✅ استهلاك الذاكرة: < 8GB للسيناريوهات الكبيرة
- ✅ قابلية التوسع: دعم ملفات حتى 1000 صفحة

#### التوافق
- ✅ دعم FDX/PDF/DOCX/TXT
- ✅ دعم النص العربي/الإنجليزي/المختلط
- ✅ دعم RTL بشكل كامل
- ✅ مخرجات بصيغ متعددة (JSON/CSV/PDF)

### معايير الوظائف / Functional Criteria

#### معالجة الملفات
- ✅ قراءة ملفات Final Draft
- ✅ استخلاص النص من PDF مع OCR
- ✅ معالجة مستندات Word
- ✅ دعم النصوص العادية

#### التحليل
- ✅ تقسيم دقيق للمشاهد والBeats
- ✅ تحديد الشخصيات والمواقع
- ✅ تحليل أسلوبي شامل
- ✅ نمذجة موضوعات متقدمة
- ✅ تحليل مشاعر هجين
- ✅ مؤشرات إنتاجية

#### الإخراج
- ✅ تقارير JSON مهيكلة
- ✅ ملفات CSV قابلة للاستيراد
- ✅ تقارير PDF مُنسقة
- ✅ تصورات تفاعلية
- ✅ واجهة تفسير مفصلة

---

## 9. الملاحظات النهائية / Final Notes

### رسالة إلى وكيل الترميز / Message to Encoding Agent

أنت الآن تملك كل المعلومات اللازمة لتنفيذ نظام الترميز المتقدم للسيناريوهات. تذكر دائماً:

#### المبادئ الأساسية / Core Principles
1. **الدقة قبل السرعة** - نتائج دقيقة أفضل من نتائج سريعة
2. **التوافق مع النص العربي** - احترام خصوصية اللغة العربية
3. **التوثيق المستمر** - وثّق كل خطوة في عملية التحليل
4. **الاختبار الشامل** - اختبر كل وظيفة بشكل منهجي
5. **التواصل الواضح** - أبلغ عن التقدم والمشاكل بوضوح

#### عند مواجهة مشكلة / When Facing Issues
```
1. توقف وفكر - لا تتسرع في الحل
2. اقرأ الخطأ بعناية - الخطأ يحتوي على المعلومات
3. ابحث في الوثائق - الإجابة غالباً موجودة
4. جرب الحلول خطوة بخطوة - لا تغير أشياء كثيرة دفعة واحدة
5. اطلب المساعدة إذا لزم الأمر - لا تتردد
```

#### نصائح للنجاح / Tips for Success
- 🎯 **ابدأ بسيط** - نفذ أبسط نسخة أولاً، ثم طوّر
- 🧪 **اختبر باستمرار** - لا تنتظر حتى النهاية
- 📝 **وثّق الأخطاء** - سجل المشاكل والحلول
- 🔄 **راجع الكود** - راجع كودك بعد كتابته
- 💡 **تعلم من الأخطاء** - كل خطأ فرصة للتعلم

#### قائمة التحقق النهائية / Final Checklist

```yaml
قبل كل تنفيذ:
  ☐ بيئة Python جاهزة مع التبعيات
  ☐ مكتبات معالجة اللغة العربية مثبتة
  ☐ نظام التسجيل مُعد
  ☐ وحدات الاختبار أولية جاهزة

قبل كل مرحلة:
  ☐ فهم متطلبات المرحلة
  ☐ إعداد خطة تنفيذية
  ☐ تحديد مؤشرات النجاح
  ☐ إعداد وحدات الاختبار

بعد كل مرحلة:
  ☐ تنفيذ الاختبارات
  ☐ التحقق من النتائج
  ☐ توثيق التغييرات
  ☐ إعداد تقرير التقدم
```

---

## ملاحظة أخيرة / Final Note

هذا النظام طموح ومعقد، لكنه قابل للتنفيذ بالكامل إذا اتبعت هذا الدليل خطوة بخطوة.

**تذكر**: أنت لست وحدك - هذا الدليل هو رفيقك في كل خطوة. ارجع إليه كلما احتجت.

**حظاً موفقاً يا وكيل الترميز! 🚀**

---

*آخر تحديث: 2025-10-01*
*الإصدار: 1.0.0*
